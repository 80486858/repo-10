{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "from matsciml.datasets import lips\n",
    "from matsciml.datasets.lips import LiPSDataset, lips_devset\n",
    "from matsciml.datasets import transforms\n",
    "import pytorch_lightning as pl\n",
    "from matsciml.models.pyg import EGNN\n",
    "from matsciml.models.base import ScalarRegressionTask, MaceEnergyForceTask\n",
    "from matsciml.lightning.data_utils import MatSciMLDataModule\n",
    "from matsciml.datasets.transforms import DistancesTransform\n",
    "from matsciml.models.pyg.mace.modules.blocks import *\n",
    "from matsciml.models.pyg.mace.modules.models import ScaleShiftMACE\n",
    "import matsciml\n",
    "import e3nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matsciml.datasets.transforms import PointCloudToGraphTransform\n",
    "# SchNet uses RBFs, and expects edge features corresponding to atom-atom distances\n",
    "dm = MatSciMLDataModule.from_devset(\n",
    "    \"LiPSDataset\",dset_kwargs={\"transforms\":[PointCloudToGraphTransform(\"pyg\", cutoff_dist=5.0)]}\n",
    ")\n",
    "# run a quick training loop\n",
    "# trainer = pl.Trainer(fast_dev_run=1000)\n",
    "# trainer.fit(task, datamodule=dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matsciml.models.pyg.mace import data, modules, tools\n",
    "from matsciml.models.pyg.mace.modules.utils import compute_mean_std_atomic_inter_energy\n",
    "from matsciml.models.pyg.mace.tools import (\n",
    "    AtomicNumberTable,\n",
    "    atomic_numbers_to_indices,\n",
    "    to_one_hot,\n",
    "    torch_geometric,\n",
    "    voigt_to_matrix,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_numpy(t: torch.Tensor) -> np.ndarray:\n",
    "    return t.cpu().detach().numpy()\n",
    "\n",
    "def compute_mean_std_atomic_inter_energy(\n",
    "    data_loader: torch.utils.data.DataLoader,\n",
    "    atomic_energies: np.ndarray,\n",
    ") -> Tuple[float, float]:\n",
    "    atomic_energies_fn = AtomicEnergiesBlock(atomic_energies=atomic_energies)\n",
    "\n",
    "    avg_atom_inter_es_list = []\n",
    "\n",
    "    for batch in data_loader:\n",
    "\n",
    "        graph = batch.get(\"graph\")\n",
    "        atomic_numbers: torch.Tensor = getattr(graph, \"atomic_numbers\")\n",
    "        z_table=tools.get_atomic_number_table_from_zs(atomic_numbers.numpy())\n",
    "\n",
    "        indices = atomic_numbers_to_indices(atomic_numbers, z_table=z_table)\n",
    "        node_attrs = to_one_hot(\n",
    "            torch.tensor(indices, dtype=torch.long).unsqueeze(-1),\n",
    "            num_classes=len(z_table))\n",
    "        node_e0 = atomic_energies_fn(node_attrs)\n",
    "        graph_e0s = scatter_sum(\n",
    "            src=node_e0, index=graph.batch, dim=-1, dim_size=graph.num_graphs\n",
    "        )\n",
    "        graph_sizes = graph.ptr[1:] - graph.ptr[:-1]\n",
    "        avg_atom_inter_es_list.append(\n",
    "            (batch['energy'] - graph_e0s) / graph_sizes\n",
    "        )  # {[n_graphs], }\n",
    "\n",
    "    avg_atom_inter_es = torch.cat(avg_atom_inter_es_list)  # [total_n_graphs]\n",
    "    mean = to_numpy(torch.mean(avg_atom_inter_es)).item()\n",
    "    std = to_numpy(torch.std(avg_atom_inter_es)).item()\n",
    "\n",
    "    return mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1231.470703125, 0.006572697777301073)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dm.setup()\n",
    "Train_loader=dm.train_dataloader()\n",
    "dataset_iter = iter(Train_loader)\n",
    "batch=next(dataset_iter)\n",
    "atomic_energies=np.array([-13.663181292231226, -1029.2809654211628, -2042.0330099956639])\n",
    "compute_mean_std_atomic_inter_energy(Train_loader,atomic_energies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = dict(\n",
    "        r_max=5.0,\n",
    "        num_bessel=8,\n",
    "        num_polynomial_cutoff=5,\n",
    "        max_ell=3,\n",
    "        interaction_cls= RealAgnosticResidualInteractionBlock ,\n",
    "        num_interactions=5,\n",
    "        num_elements=3,\n",
    "        hidden_irreps=e3nn.o3.Irreps('16x0e+16x1o+16x2e'),\n",
    "        atomic_energies=torch.Tensor([-13.663181292231226, -1029.2809654211628, -2042.0330099956639]),\n",
    "        avg_num_neighbors=14.38,\n",
    "        atomic_numbers=(1,6,8),\n",
    "        correlation=3,\n",
    "        gate=torch.nn.functional.silu,\n",
    "        interaction_cls_first=RealAgnosticResidualInteractionBlock,\n",
    "        MLP_irreps=e3nn.o3.Irreps('16x0e'),\n",
    "        atomic_inter_scale=0.006573571357876062,\n",
    "        atomic_inter_shift=1231.4705810546875\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DIKSHIKA\\miniconda3\\envs\\matsciml\\lib\\site-packages\\torch\\jit\\_check.py:172: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\"The TorchScript type system doesn't support \"\n",
      "f:\\Vaibhav\\M3RG\\matsciml\\matsciml\\models\\pyg\\mace\\modules\\blocks.py:131: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(atomic_energies, dtype=torch.get_default_dtype()),\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 1 required positional argument: 'output_dim'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [17]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m task \u001b[38;5;241m=\u001b[39m \u001b[43mMaceEnergyForceTask\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mScaleShiftMACE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# kwargs to be passed into the creation of SchNet model\u001b[39;49;00m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# which keys to use as targets\u001b[39;49;00m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtask_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43menergy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforce\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mf:\\Vaibhav\\M3RG\\matsciml\\matsciml\\models\\base.py:1203\u001b[0m, in \u001b[0;36mMaceEnergyForceTask.__init__\u001b[1;34m(self, encoder, encoder_class, encoder_kwargs, loss_func, task_keys, output_kwargs, **kwargs)\u001b[0m\n\u001b[0;32m   1193\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m   1194\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1195\u001b[0m     encoder: Optional[nn\u001b[38;5;241m.\u001b[39mModule] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1201\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m   1202\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1203\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m   1204\u001b[0m         encoder,\n\u001b[0;32m   1205\u001b[0m         encoder_class,\n\u001b[0;32m   1206\u001b[0m         encoder_kwargs,\n\u001b[0;32m   1207\u001b[0m         loss_func,\n\u001b[0;32m   1208\u001b[0m         task_keys,\n\u001b[0;32m   1209\u001b[0m         output_kwargs,\n\u001b[0;32m   1210\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   1211\u001b[0m     )\n\u001b[0;32m   1212\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_hyperparameters(ignore\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoder\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss_func\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[1;32mf:\\Vaibhav\\M3RG\\matsciml\\matsciml\\models\\base.py:696\u001b[0m, in \u001b[0;36mBaseTaskModule.__init__\u001b[1;34m(self, encoder, encoder_class, encoder_kwargs, loss_func, task_keys, output_kwargs, lr, weight_decay, normalize_kwargs, scheduler_kwargs, **kwargs)\u001b[0m\n\u001b[0;32m    694\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_kwargs \u001b[38;5;241m=\u001b[39m default_heads\n\u001b[0;32m    695\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnormalize_kwargs \u001b[38;5;241m=\u001b[39m normalize_kwargs\n\u001b[1;32m--> 696\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtask_keys \u001b[38;5;241m=\u001b[39m task_keys\n\u001b[0;32m    697\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_hyperparameters(ignore\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoder\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss_func\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\DIKSHIKA\\miniconda3\\envs\\matsciml\\lib\\site-packages\\torch\\nn\\modules\\module.py:1674\u001b[0m, in \u001b[0;36mModule.__setattr__\u001b[1;34m(self, name, value)\u001b[0m\n\u001b[0;32m   1672\u001b[0m     buffers[name] \u001b[38;5;241m=\u001b[39m value\n\u001b[0;32m   1673\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1674\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__setattr__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mf:\\Vaibhav\\M3RG\\matsciml\\matsciml\\models\\base.py:723\u001b[0m, in \u001b[0;36mBaseTaskModule.task_keys\u001b[1;34m(self, values)\u001b[0m\n\u001b[0;32m    720\u001b[0m \u001b[38;5;66;03m# if we're setting task keys we have enough to initialize\u001b[39;00m\n\u001b[0;32m    721\u001b[0m \u001b[38;5;66;03m# the output heads\u001b[39;00m\n\u001b[0;32m    722\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_initialized:\n\u001b[1;32m--> 723\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_heads \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_output_heads\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    724\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnormalizers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_normalizers()\n\u001b[0;32m    725\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtask_keys\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_keys\n",
      "File \u001b[1;32mf:\\Vaibhav\\M3RG\\matsciml\\matsciml\\models\\base.py:1253\u001b[0m, in \u001b[0;36mMaceEnergyForceTask._make_output_heads\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1251\u001b[0m modules \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m   1252\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtask_keys:\n\u001b[1;32m-> 1253\u001b[0m     modules[key] \u001b[38;5;241m=\u001b[39m OutputHead(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_kwargs)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m   1254\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m nn\u001b[38;5;241m.\u001b[39mModuleDict(modules)\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'output_dim'"
     ]
    }
   ],
   "source": [
    "# task = ScalarRegressionTask(\n",
    "#     encoder_class=ScaleShiftMACE,\n",
    "#     # kwargs to be passed into the creation of SchNet model\n",
    "#     encoder_kwargs=model_config,\n",
    "#     # which keys to use as targets\n",
    "#     task_keys=[\"energy\"],\n",
    "#     output_kwargs={\n",
    "#       \"block_type\": \"IrrepOutputBlock\",\n",
    "#       \"input_dim\": \"0e\", \n",
    "#       \"hidden_dim\": \"30x0e + 10x1e\", \n",
    "#       \"output_dim\": \"0e\",\n",
    "#       \"activation\": [\"torch.nn.SiLU\", None],\n",
    "#       \"residual\":False\n",
    "#     }\n",
    "# )\n",
    "\n",
    "task = ScalarRegressionTask(\n",
    "    encoder_class=ScaleShiftMACE,\n",
    "    # kwargs to be passed into the creation of SchNet model\n",
    "    encoder_kwargs=model_config,\n",
    "    # which keys to use as targets\n",
    "    task_keys=[\"energy\",\"force\"],\n",
    "    output_kwargs={\n",
    "      \"block_type\": \"IdentityOutputBlock\",\n",
    "      \"output_dim\": 1,\n",
    "      \"hidden_dim\": None\n",
    "\n",
    "      }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ScalarRegressionTask(\n",
       "  (encoder): ScaleShiftMACE(\n",
       "    (atom_embedding): Embedding(100, 16, padding_idx=0)\n",
       "    (node_embedding): LinearNodeEmbeddingBlock(\n",
       "      (linear): Linear(3x0e -> 16x0e | 48 weights)\n",
       "    )\n",
       "    (radial_embedding): RadialEmbeddingBlock(\n",
       "      (bessel_fn): BesselBasis(r_max=5.0, num_basis=8, trainable=False)\n",
       "      (cutoff_fn): PolynomialCutoff(p=5.0, r_max=5.0)\n",
       "    )\n",
       "    (spherical_harmonics): SphericalHarmonics()\n",
       "    (atomic_energies_fn): AtomicEnergiesBlock(energies=[-13.6632, -1029.2810, -2042.0330])\n",
       "    (interactions): ModuleList(\n",
       "      (0): RealAgnosticResidualInteractionBlock(\n",
       "        (linear_up): Linear(16x0e -> 16x0e | 256 weights)\n",
       "        (conv_tp): TensorProduct(16x0e x 1x0e+1x1o+1x2e+1x3o -> 16x0e+16x1o+16x2e+16x3o | 64 paths | 64 weights)\n",
       "        (conv_tp_weights): FullyConnectedNet[8, 64, 64, 64, 64]\n",
       "        (linear): Linear(16x0e+16x1o+16x2e+16x3o -> 16x0e+16x1o+16x2e+16x3o | 1024 weights)\n",
       "        (skip_tp): FullyConnectedTensorProduct(16x0e x 3x0e -> 16x0e+16x1o+16x2e | 768 paths | 768 weights)\n",
       "        (reshape): reshape_irreps()\n",
       "      )\n",
       "      (1-3): 3 x RealAgnosticResidualInteractionBlock(\n",
       "        (linear_up): Linear(16x0e+16x1o+16x2e -> 16x0e+16x1o+16x2e | 768 weights)\n",
       "        (conv_tp): TensorProduct(16x0e+16x1o+16x2e x 1x0e+1x1o+1x2e+1x3o -> 48x0e+80x1o+80x2e+64x3o | 272 paths | 272 weights)\n",
       "        (conv_tp_weights): FullyConnectedNet[8, 64, 64, 64, 272]\n",
       "        (linear): Linear(48x0e+80x1o+80x2e+64x3o -> 16x0e+16x1o+16x2e+16x3o | 4352 weights)\n",
       "        (skip_tp): FullyConnectedTensorProduct(16x0e+16x1o+16x2e x 3x0e -> 16x0e+16x1o+16x2e | 2304 paths | 2304 weights)\n",
       "        (reshape): reshape_irreps()\n",
       "      )\n",
       "      (4): RealAgnosticResidualInteractionBlock(\n",
       "        (linear_up): Linear(16x0e+16x1o+16x2e -> 16x0e+16x1o+16x2e | 768 weights)\n",
       "        (conv_tp): TensorProduct(16x0e+16x1o+16x2e x 1x0e+1x1o+1x2e+1x3o -> 48x0e+80x1o+80x2e+64x3o | 272 paths | 272 weights)\n",
       "        (conv_tp_weights): FullyConnectedNet[8, 64, 64, 64, 272]\n",
       "        (linear): Linear(48x0e+80x1o+80x2e+64x3o -> 16x0e+16x1o+16x2e+16x3o | 4352 weights)\n",
       "        (skip_tp): FullyConnectedTensorProduct(16x0e+16x1o+16x2e x 3x0e -> 16x0e | 768 paths | 768 weights)\n",
       "        (reshape): reshape_irreps()\n",
       "      )\n",
       "    )\n",
       "    (products): ModuleList(\n",
       "      (0-3): 4 x EquivariantProductBasisBlock(\n",
       "        (symmetric_contractions): SymmetricContraction(\n",
       "          (contractions): ModuleList(\n",
       "            (0): Contraction(\n",
       "              (contractions_weighting): ModuleList(\n",
       "                (0-1): 2 x GraphModule()\n",
       "              )\n",
       "              (contractions_features): ModuleList(\n",
       "                (0-1): 2 x GraphModule()\n",
       "              )\n",
       "              (weights): ParameterList(\n",
       "                  (0): Parameter containing: [torch.float32 of size 3x4x16]\n",
       "                  (1): Parameter containing: [torch.float32 of size 3x1x16]\n",
       "              )\n",
       "              (graph_opt_main): GraphModule()\n",
       "            )\n",
       "            (1): Contraction(\n",
       "              (contractions_weighting): ModuleList(\n",
       "                (0-1): 2 x GraphModule()\n",
       "              )\n",
       "              (contractions_features): ModuleList(\n",
       "                (0-1): 2 x GraphModule()\n",
       "              )\n",
       "              (weights): ParameterList(\n",
       "                  (0): Parameter containing: [torch.float32 of size 3x6x16]\n",
       "                  (1): Parameter containing: [torch.float32 of size 3x1x16]\n",
       "              )\n",
       "              (graph_opt_main): GraphModule()\n",
       "            )\n",
       "            (2): Contraction(\n",
       "              (contractions_weighting): ModuleList(\n",
       "                (0-1): 2 x GraphModule()\n",
       "              )\n",
       "              (contractions_features): ModuleList(\n",
       "                (0-1): 2 x GraphModule()\n",
       "              )\n",
       "              (weights): ParameterList(\n",
       "                  (0): Parameter containing: [torch.float32 of size 3x7x16]\n",
       "                  (1): Parameter containing: [torch.float32 of size 3x1x16]\n",
       "              )\n",
       "              (graph_opt_main): GraphModule()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (linear): Linear(16x0e+16x1o+16x2e -> 16x0e+16x1o+16x2e | 768 weights)\n",
       "      )\n",
       "      (4): EquivariantProductBasisBlock(\n",
       "        (symmetric_contractions): SymmetricContraction(\n",
       "          (contractions): ModuleList(\n",
       "            (0): Contraction(\n",
       "              (contractions_weighting): ModuleList(\n",
       "                (0-1): 2 x GraphModule()\n",
       "              )\n",
       "              (contractions_features): ModuleList(\n",
       "                (0-1): 2 x GraphModule()\n",
       "              )\n",
       "              (weights): ParameterList(\n",
       "                  (0): Parameter containing: [torch.float32 of size 3x4x16]\n",
       "                  (1): Parameter containing: [torch.float32 of size 3x1x16]\n",
       "              )\n",
       "              (graph_opt_main): GraphModule()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (linear): Linear(16x0e -> 16x0e | 256 weights)\n",
       "      )\n",
       "    )\n",
       "    (readouts): ModuleList(\n",
       "      (0-3): 4 x LinearReadoutBlock(\n",
       "        (linear): Linear(16x0e+16x1o+16x2e -> 1x0e | 16 weights)\n",
       "      )\n",
       "      (4): NonLinearReadoutBlock(\n",
       "        (linear_1): Linear(16x0e -> 16x0e | 256 weights)\n",
       "        (non_linearity): Activation [x] (16x0e -> 16x0e)\n",
       "        (linear_2): Linear(16x0e -> 1x0e | 16 weights)\n",
       "      )\n",
       "    )\n",
       "    (scale_shift): ScaleShiftBlock(scale=0.006574, shift=1231.470581)\n",
       "  )\n",
       "  (loss_func): MSELoss()\n",
       "  (output_heads): ModuleDict(\n",
       "    (energy): OutputHead(\n",
       "      (blocks): Sequential(\n",
       "        (0): IdentityOutputBlock()\n",
       "        (1): IdentityOutputBlock()\n",
       "        (2): IdentityOutputBlock()\n",
       "      )\n",
       "    )\n",
       "    (force): OutputHead(\n",
       "      (blocks): Sequential(\n",
       "        (0): IdentityOutputBlock()\n",
       "        (1): IdentityOutputBlock()\n",
       "        (2): IdentityOutputBlock()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\DIKSHIKA\\miniconda3\\envs\\matsciml\\lib\\site-packages\\pytorch_lightning\\trainer\\setup.py:175: PossibleUserWarning: GPU available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='gpu', devices=1)`.\n",
      "  rank_zero_warn(\n",
      "\n",
      "  | Name         | Type           | Params\n",
      "------------------------------------------------\n",
      "0 | encoder      | ScaleShiftMACE | 184 K \n",
      "1 | loss_func    | MSELoss        | 0     \n",
      "2 | output_heads | ModuleDict     | 0     \n",
      "------------------------------------------------\n",
      "184 K     Trainable params\n",
      "0         Non-trainable params\n",
      "184 K     Total params\n",
      "0.739     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DIKSHIKA\\miniconda3\\envs\\matsciml\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]True\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m trainer \u001b[38;5;241m=\u001b[39m pl\u001b[38;5;241m.\u001b[39mTrainer(max_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdm\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\DIKSHIKA\\miniconda3\\envs\\matsciml\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:603\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    601\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`Trainer.fit()` requires a `LightningModule`, got: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    602\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m_lightning_module \u001b[38;5;241m=\u001b[39m model\n\u001b[1;32m--> 603\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    604\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[0;32m    605\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\DIKSHIKA\\miniconda3\\envs\\matsciml\\lib\\site-packages\\pytorch_lightning\\trainer\\call.py:38\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[1;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 38\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[0;32m     41\u001b[0m     trainer\u001b[38;5;241m.\u001b[39m_call_teardown_hook()\n",
      "File \u001b[1;32mc:\\Users\\DIKSHIKA\\miniconda3\\envs\\matsciml\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:645\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    638\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m ckpt_path \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresume_from_checkpoint\n\u001b[0;32m    639\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_set_ckpt_path(\n\u001b[0;32m    640\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[0;32m    641\u001b[0m     ckpt_path,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    642\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    643\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    644\u001b[0m )\n\u001b[1;32m--> 645\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    647\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[0;32m    648\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\DIKSHIKA\\miniconda3\\envs\\matsciml\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1098\u001b[0m, in \u001b[0;36mTrainer._run\u001b[1;34m(self, model, ckpt_path)\u001b[0m\n\u001b[0;32m   1094\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39mrestore_training_state()\n\u001b[0;32m   1096\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39mresume_end()\n\u001b[1;32m-> 1098\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1100\u001b[0m log\u001b[38;5;241m.\u001b[39mdetail(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1101\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_teardown()\n",
      "File \u001b[1;32mc:\\Users\\DIKSHIKA\\miniconda3\\envs\\matsciml\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1177\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredicting:\n\u001b[0;32m   1176\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_predict()\n\u001b[1;32m-> 1177\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\DIKSHIKA\\miniconda3\\envs\\matsciml\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1190\u001b[0m, in \u001b[0;36mTrainer._run_train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1187\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pre_training_routine()\n\u001b[0;32m   1189\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m isolate_rng():\n\u001b[1;32m-> 1190\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_sanity_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;66;03m# enable train mode\u001b[39;00m\n\u001b[0;32m   1193\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\DIKSHIKA\\miniconda3\\envs\\matsciml\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1262\u001b[0m, in \u001b[0;36mTrainer._run_sanity_check\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1260\u001b[0m \u001b[38;5;66;03m# run eval step\u001b[39;00m\n\u001b[0;32m   1261\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m-> 1262\u001b[0m     \u001b[43mval_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1264\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_callback_hooks(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_sanity_check_end\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1266\u001b[0m \u001b[38;5;66;03m# reset logger connector\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\DIKSHIKA\\miniconda3\\envs\\matsciml\\lib\\site-packages\\pytorch_lightning\\loops\\loop.py:199\u001b[0m, in \u001b[0;36mLoop.run\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    197\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    198\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 199\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madvance(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    200\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n\u001b[0;32m    201\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\DIKSHIKA\\miniconda3\\envs\\matsciml\\lib\\site-packages\\pytorch_lightning\\loops\\dataloader\\evaluation_loop.py:152\u001b[0m, in \u001b[0;36mEvaluationLoop.advance\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_dataloaders \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    151\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataloader_idx\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m dataloader_idx\n\u001b[1;32m--> 152\u001b[0m dl_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepoch_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_fetcher\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdl_max_batches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;66;03m# store batch level output per dataloader\u001b[39;00m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_outputs\u001b[38;5;241m.\u001b[39mappend(dl_outputs)\n",
      "File \u001b[1;32mc:\\Users\\DIKSHIKA\\miniconda3\\envs\\matsciml\\lib\\site-packages\\pytorch_lightning\\loops\\loop.py:199\u001b[0m, in \u001b[0;36mLoop.run\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    197\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    198\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 199\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madvance(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    200\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n\u001b[0;32m    201\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\DIKSHIKA\\miniconda3\\envs\\matsciml\\lib\\site-packages\\pytorch_lightning\\loops\\epoch\\evaluation_epoch_loop.py:137\u001b[0m, in \u001b[0;36mEvaluationEpochLoop.advance\u001b[1;34m(self, data_fetcher, dl_max_batches, kwargs)\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mincrement_started()\n\u001b[0;32m    136\u001b[0m \u001b[38;5;66;03m# lightning module methods\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_evaluation_step(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_evaluation_step_end(output)\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mincrement_processed()\n",
      "File \u001b[1;32mc:\\Users\\DIKSHIKA\\miniconda3\\envs\\matsciml\\lib\\site-packages\\pytorch_lightning\\loops\\epoch\\evaluation_epoch_loop.py:234\u001b[0m, in \u001b[0;36mEvaluationEpochLoop._evaluation_step\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"The evaluation step (validation_step or test_step depending on the trainer's state).\u001b[39;00m\n\u001b[0;32m    224\u001b[0m \n\u001b[0;32m    225\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    231\u001b[0m \u001b[38;5;124;03m    the outputs of the step\u001b[39;00m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    233\u001b[0m hook_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_step\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mtesting \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation_step\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 234\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_strategy_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhook_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    236\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[1;32mc:\\Users\\DIKSHIKA\\miniconda3\\envs\\matsciml\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1480\u001b[0m, in \u001b[0;36mTrainer._call_strategy_hook\u001b[1;34m(self, hook_name, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1477\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m   1479\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m-> 1480\u001b[0m     output \u001b[38;5;241m=\u001b[39m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1482\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[0;32m   1483\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "File \u001b[1;32mc:\\Users\\DIKSHIKA\\miniconda3\\envs\\matsciml\\lib\\site-packages\\pytorch_lightning\\strategies\\strategy.py:390\u001b[0m, in \u001b[0;36mStrategy.validation_step\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    388\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprecision_plugin\u001b[38;5;241m.\u001b[39mval_step_context():\n\u001b[0;32m    389\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, ValidationStep)\n\u001b[1;32m--> 390\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mvalidation_step(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mf:\\Vaibhav\\M3RG\\matsciml\\matsciml\\models\\base.py:951\u001b[0m, in \u001b[0;36mBaseTaskModule.validation_step\u001b[1;34m(self, batch, batch_idx)\u001b[0m\n\u001b[0;32m    946\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvalidation_step\u001b[39m(\n\u001b[0;32m    947\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    948\u001b[0m     batch: Dict[\u001b[38;5;28mstr\u001b[39m, Union[torch\u001b[38;5;241m.\u001b[39mTensor, dgl\u001b[38;5;241m.\u001b[39mDGLGraph, Dict[\u001b[38;5;28mstr\u001b[39m, torch\u001b[38;5;241m.\u001b[39mTensor]]],\n\u001b[0;32m    949\u001b[0m     batch_idx: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m    950\u001b[0m ):\n\u001b[1;32m--> 951\u001b[0m     loss_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compute_losses\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    952\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    953\u001b[0m     \u001b[38;5;66;03m# prepending training flag for\u001b[39;00m\n",
      "File \u001b[1;32mf:\\Vaibhav\\M3RG\\matsciml\\matsciml\\models\\base.py:895\u001b[0m, in \u001b[0;36mBaseTaskModule._compute_losses\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    875\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    876\u001b[0m \u001b[38;5;124;03mCompute pred versus target for every target, then sum.\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    892\u001b[0m \u001b[38;5;124;03m    containing each individual target loss.\u001b[39;00m\n\u001b[0;32m    893\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    894\u001b[0m targets \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_targets(batch)\n\u001b[1;32m--> 895\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    896\u001b[0m losses \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    897\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtask_keys:\n",
      "File \u001b[1;32mc:\\Users\\DIKSHIKA\\miniconda3\\envs\\matsciml\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mf:\\Vaibhav\\M3RG\\matsciml\\matsciml\\models\\base.py:792\u001b[0m, in \u001b[0;36mBaseTaskModule.forward\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    790\u001b[0m     embedding \u001b[38;5;241m=\u001b[39m batch\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    791\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 792\u001b[0m     embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    793\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_embedding(embedding)\n\u001b[0;32m    794\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[1;32mc:\\Users\\DIKSHIKA\\miniconda3\\envs\\matsciml\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mf:\\Vaibhav\\M3RG\\matsciml\\matsciml\\models\\base.py:292\u001b[0m, in \u001b[0;36mAbstractTask.forward\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    274\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    275\u001b[0m \u001b[38;5;124;03mGiven a batch structure, extract out data and pass it into the\u001b[39;00m\n\u001b[0;32m    276\u001b[0m \u001b[38;5;124;03mneural network architecture. This implements the 'forward' method\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    289\u001b[0m \u001b[38;5;124;03m    Data structure containing system/graph and point/node level embeddings.\u001b[39;00m\n\u001b[0;32m    290\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    291\u001b[0m input_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread_batch(batch)\n\u001b[1;32m--> 292\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minput_data)\n\u001b[0;32m    293\u001b[0m \u001b[38;5;66;03m# raise an error to help spot models that have not yet been refactored\u001b[39;00m\n\u001b[0;32m    294\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, Embeddings):\n",
      "File \u001b[1;32mf:\\Vaibhav\\M3RG\\matsciml\\matsciml\\models\\pyg\\mace\\modules\\models.py:361\u001b[0m, in \u001b[0;36mScaleShiftMACE._forward\u001b[1;34m(self, data, training, compute_force, compute_virials, compute_stress, compute_displacement)\u001b[0m\n\u001b[0;32m    359\u001b[0m \u001b[38;5;28mprint\u001b[39m(data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpositions\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mrequires_grad)\n\u001b[0;32m    360\u001b[0m \u001b[38;5;28mprint\u001b[39m()\n\u001b[1;32m--> 361\u001b[0m forces, virials, stress \u001b[38;5;241m=\u001b[39m \u001b[43mget_outputs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    362\u001b[0m \u001b[43m    \u001b[49m\u001b[43menergy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minter_e\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    363\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpositions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpositions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    364\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdisplacement\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisplacement\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    365\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcell\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    366\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    367\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompute_force\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompute_force\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    368\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompute_virials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompute_virials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    369\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompute_stress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompute_stress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    370\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    371\u001b[0m output \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    372\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menergy\u001b[39m\u001b[38;5;124m\"\u001b[39m: total_energy,\n\u001b[0;32m    373\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnode_energy\u001b[39m\u001b[38;5;124m\"\u001b[39m: node_energy,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    380\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124medge_feats\u001b[39m\u001b[38;5;124m\"\u001b[39m :edge_feats\n\u001b[0;32m    381\u001b[0m }\n\u001b[0;32m    383\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Embeddings(output[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menergy\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m),output[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforces\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[1;32mf:\\Vaibhav\\M3RG\\matsciml\\matsciml\\models\\pyg\\mace\\modules\\utils.py:134\u001b[0m, in \u001b[0;36mget_outputs\u001b[1;34m(energy, positions, displacement, cell, training, compute_force, compute_virials, compute_stress)\u001b[0m\n\u001b[0;32m    124\u001b[0m     forces, virials, stress \u001b[38;5;241m=\u001b[39m compute_forces_virials(\n\u001b[0;32m    125\u001b[0m         energy\u001b[38;5;241m=\u001b[39menergy,\n\u001b[0;32m    126\u001b[0m         positions\u001b[38;5;241m=\u001b[39mpositions,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    130\u001b[0m         training\u001b[38;5;241m=\u001b[39mtraining,\n\u001b[0;32m    131\u001b[0m     )\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m compute_force:\n\u001b[0;32m    133\u001b[0m     forces, virials, stress \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 134\u001b[0m         \u001b[43mcompute_forces\u001b[49m\u001b[43m(\u001b[49m\u001b[43menergy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menergy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpositions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpositions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m    135\u001b[0m         \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    136\u001b[0m         \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    137\u001b[0m     )\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    139\u001b[0m     forces, virials, stress \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[1;32mf:\\Vaibhav\\M3RG\\matsciml\\matsciml\\models\\pyg\\mace\\modules\\utils.py:25\u001b[0m, in \u001b[0;36mcompute_forces\u001b[1;34m(energy, positions, training)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_forces\u001b[39m(\n\u001b[0;32m     22\u001b[0m     energy: torch\u001b[38;5;241m.\u001b[39mTensor, positions: torch\u001b[38;5;241m.\u001b[39mTensor, training: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     23\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m     24\u001b[0m     grad_outputs: List[Optional[torch\u001b[38;5;241m.\u001b[39mTensor]] \u001b[38;5;241m=\u001b[39m [torch\u001b[38;5;241m.\u001b[39mones_like(energy)]\n\u001b[1;32m---> 25\u001b[0m     gradient \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43menergy\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# [n_graphs, ]\u001b[39;49;00m\n\u001b[0;32m     27\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mpositions\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# [n_nodes, 3]\u001b[39;49;00m\n\u001b[0;32m     28\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Make sure the graph is not destroyed during training\u001b[39;49;00m\n\u001b[0;32m     30\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Create graph for second derivative\u001b[39;49;00m\n\u001b[0;32m     31\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# For complete dissociation turn to true\u001b[39;49;00m\n\u001b[0;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m[\n\u001b[0;32m     33\u001b[0m         \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     34\u001b[0m     ]  \u001b[38;5;66;03m# [n_nodes, 3]\u001b[39;00m\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gradient \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     36\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mzeros_like(positions)\n",
      "File \u001b[1;32mc:\\Users\\DIKSHIKA\\miniconda3\\envs\\matsciml\\lib\\site-packages\\torch\\autograd\\__init__.py:303\u001b[0m, in \u001b[0;36mgrad\u001b[1;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched)\u001b[0m\n\u001b[0;32m    301\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _vmap_internals\u001b[38;5;241m.\u001b[39m_vmap(vjp, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, allow_none_pass_through\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)(grad_outputs_)\n\u001b[0;32m    302\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 303\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    304\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_outputs_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    305\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(max_epochs=100)\n",
    "trainer.fit(task, datamodule=dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matsciml.models.pyg.mace.modules.models import ScaleShiftMACE1\n",
    "model=ScaleShiftMACE(**model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm.setup()\n",
    "Train_loader=dm.train_dataloader()\n",
    "dataset_iter = iter(Train_loader)\n",
    "batch=next(dataset_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  4,   6,   6,  ..., 663, 663, 663],\n",
       "        [  3,   1,   2,  ..., 655, 660, 661]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['graph'].edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "out=model(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch['graph']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs=model.read_batch(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs=model._forward(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "matsciml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
